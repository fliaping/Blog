+++
author = "Payne Xu"
categories = ["搜索引擎"]
date = 2016-05-15T11:55:14Z
description = ""
draft = false
slug = "introduction-of-vertical-search-engine"
tags = ["搜索引擎"]
title = "垂直搜索引擎基础知识"

+++



# 引言/废话
搜索引擎是大家平时使用最广泛的网络应用之一，它是普通网民接触互联网的入口，也是网络信息的搜集系统，其重要性不言而喻。我认为一个优秀的搜索服务应该实现一些基本的要求：

* 准确回应用户搜索目的
* 提供公正的结果排序

然而，目前最大的中文搜索引擎，在经历了[血友病吧事件](https://zh.wikipedia.org/wiki/百度贴吧#2016.E5.B9.B4.E8.A1.80.E5.8F.8B.E7.97.85.E5.90.A7.E4.BA.8B.E4.BB.B6)、[魏则西事件](https://zh.wikipedia.org/zh/魏则西事件)之后,网民已经失去对其的信任，甚至很多人产生了厌恶之情，且不说是否有人落井下石，或者说是罪有应得，总之，它带给大家的是不好的用户体验。吐槽到此为止，从这里开始我们进入正题。

<!--more-->


# 搜索引擎基本原理

众所周知，我们浏览的网页其实是一个个文本文件（动态生成的也算，总之浏览器接受到的就是文本），并且这些文本来自于不同的站点和服务器，例如新浪网在新浪的服务器上。截止2014年，[全球互联网站点数量超过十亿](http://www.chinanews.com/gj/2014/09-18/6604823.shtml)，大部分站点之间数据没有关联，我们如何能在这十亿多的站点上查到我们需要的信息？总不能一个一个站点询问吧，因此我们需要一个包含这十亿多站点信息的Master来告诉我们结果，这个Master就可以看做搜索引擎，它的基本性质有如下几点：

* 基本包含整个互联网的信息
* 对信息进行了初步的整理
* 能及时更新信息，达到基本和源站点信息同步
* 能在短时间回应查询请求

按照[Wikipedia-搜索引擎](https://zh.wikipedia.org/wiki/搜索引擎)的步骤及分类方法，分为：

搜集信息 -> 整理信息 -> 接受查询

其中『整理信息』及『接受查询』的过程，大量应用了[文本信息检索技术](https://zh.wikipedia.org/wiki/文本信息检索)，并根据网络超文本的特点，引入了更多的信息。

这里不引用Wikipedia上的原文，我对这些概念进行一些简化和总结。
## 搜集信息
这个Master如何才能包含整个互联网的信息？它没有权利直接访问站点数据存储系统，那么它就只能通过模拟用户浏览来把网页保存下来，但浏览网页是要通过URL来定位的，它原本没有所有链接的列表，但通过[网络爬虫（Web spider/Web crawler）](https://zh.wikipedia.org/wiki/網路蜘蛛)就可以遍历所有网页，当然这个过程基本都是自动的。因为网页的访问时基于HTTP（Hypertext Transfer Protocol），意为超文本传输协议，这里的超文本指文本页面中包含有指向到其他页面的链接，网页通过超链接彼此联系。所以理想状态下，爬虫利用少量的起始页面，通过解析已访问页面中的超链接，经过N次跳转即可访问到大部分页面。那么简单的说，搜集信息的这个步骤就是利用网络爬虫通过自动化方式下载网页。
## 整理信息
如果不进行信息整理，那么每次查找就需要遍历所有页面，这显然和去所有站点查询一遍没什么区别，时间代价大的难以接受。因此我们需要对搜集来的信息建立目录，按照一定的规则对其进行编排，使其变得有序。这个过程叫做『创建索引』。
## 接受查询
用户向搜索引擎发出查询请求，搜索引擎分析请求，在创建的索引库中进行查询，之后返回给用户结果。在当前这个步骤中，我们要考虑的是搜索引擎的性能问题，因为可能有很多人同时进行查询，并且要保证用户的查询能快速回应，这不是一件简单的事情。

# 搜索引擎分类
>搜索引擎按其工作方式主要可分为三种，分别是全文搜索引擎（Full Text Search Engine）、垂直搜索引擎（Vertical Search Engine）和元搜索引擎（Meta Search Engine）。

## 全文搜索
通常我们用到的例如Google、Bing、Yahoo、Baidu等都属于全文搜索，即通用搜索引擎。它们提取各个站点中的整个页面信息而创建索引，通过用户查询的关键字匹配相关记录，对结果进行排序并返回给用户，一般展示格式为网页标题和摘要信息，其查询的范围是所有网页。
## 垂直搜索
垂直搜索是针对某一行业的专业搜索引擎，是搜索引擎的细分和延伸，是对网页库中的某类专门的信息进行一次集成，定向分字段抽取出需要的数据进行处理后再以某种形式返回给用户。这里的形式可能各不相同，结果也更加精准。解决了通用搜索中信息量大、查询不准确、深度不够等问题，但其信息范围局限于某一特定领域、人群、需求。常见的垂直搜索有学术论文搜索、旅游信息搜索、周边信息搜索等等。
## 元搜索
元搜索引擎在接受用户查询请求时，同时在其他多个引擎上进行搜索，并将结果返回给用户。按照作者理解，元搜索不是真正的搜索引擎，并且当前元搜索的并没有什么优势，几乎见不到了。

# 垂直搜索特点
前面简要介绍了搜索引擎的基本知识，这里该进入本文的重点--垂直搜索。
## 信息采集的特点
从采集方式看，通用搜索以被动方式为主，搜索引擎和被采集的网页没有约定的、标准的格式；站内搜索以主动方式为主，被采集的办公文档、CRM和ERP中的数据等都和企业搜索引擎有着约定好的采集接口和安全接口；垂直搜索则采用被动和主动想结合的方式，通过主动方式，有效采集网页中标引的元数据，整合上下游网页资源或者商业数据库，提供更加准确的搜索服务。通用搜索采用广度为先的策略，所以对采集深度要求不高，而垂直搜索和站内搜索需要挖掘出行业内所有相关的网页信息，所以采用深度为先的策略，同时由于行业内的一些有商业价值的信息采用动态发布的方式，所以垂直搜索对动态网页的采集优先级别较高。实际应用中，垂直搜索和站内搜索都需要集成和采集关系数据库中的结构化信息。
## 信息整理的特点
垂直搜索和普通的网页搜索的最大区别是对网页信息进行了结构化信息抽取加工，也就是将网页的非结构化数据抽取成特定的结构化信息数据，好比网页搜索是以网页为最小单位，而垂直搜索是以结构化数据为最小单位。垂直搜索的结构化信息提取和加工主要包括：网页元数据的提取和内容中结构化实体信息的提取。这些数据存储到数据库中，进行进一步的加工处理，如：去重、分类等，最后分词、索引再以搜索的方式满足用户的需求。目前，大部分垂直搜索的结构化信息提取都是依靠手工、半手工的方式来完成的，面对互联网的海量信息，很难保证信息的实时性和有效性，因此智能化成为垂直搜索的发展趋势。
## 查询结果的特点
从信息检索的结果来看，垂直搜索引擎不但能够对网页信息中的结构化信息进行检索，而且能够提供结构化和非结构化信息相结合的检索方式。从检索结果的排序方式看，垂直搜索的排序需求更加多样化。

# 如何构建垂直搜索
## 技术流程
![](https://o364p1r5a.qnssl.com/blog/14634008481848.jpg)
从上图可以看出，搜索引擎的基本技术流程，其中有三大核心组件

* 索引库(DB)是信息存储的地方，这里的信息已经变成倒排结构。* 索引器(Indexer)是生成索引的模块，我们将处理过后的信息包装成文档交给索引器，索引器会在索引库中建立该文档的索引（也就是倒排结构）。* 检索器（Searcher）是信息查询的模块

## 具体实现

搜索引擎的每一个模块都是作为一个单独的研究方向，我们不可能在短时间从无到有实现整个搜索引擎，幸好有开源社区的帮助，我们不用重复造轮子，将造好的轮子组装起来就好了。这里我们用到的开源软件是Apache的 [Solr](https://lucene.apache.org/solr)，它是基于[Lucene](https://lucene.apache.org/)的企业级搜索服务器，关于Solr和Lucene的介绍后面会讲到。

下图是我们利用Solr构建搜索引擎的基本构架，整个Solr可以看做一个完整的服务，Indexer模块来更新Solr的数据；Webapp从Solr获取数据，经过一些处理返回到浏览器，呈现给用户。
![](https://o364p1r5a.qnssl.com/blog/14634555420272.jpg)

## 用到的开源项目

**`Lucene`**

**`Solr`**

**`Heritrix`**

**`webMagic`**

**`Zookeeper`**

**`AngularJS`**

**`Ionic`**

